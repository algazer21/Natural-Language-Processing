{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jlY-3RdFVWKa",
        "ZbD1WyyOlkAq",
        "_vLpxDWdp6L3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tarea 5: Modelos de lenguaje neuronales**\n",
        "##**Procesamiento de lenguaje natural**\n",
        "## **Alan García Zermeño**\n",
        "### 18/04/2023"
      ],
      "metadata": {
        "id": "69SYxQ0BA8yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usamos el código utilizado para la realización de la práctica 5 y 6 de procesamiento de lenguaje: https://colab.research.google.com/drive/1zvqLxKW7yxbpbh6zUqRk-55HnzR1o-lz?usp=sharing\n",
        "### Usaremos los siguientes directorios en la nube para realizar el presente trabajo:\n",
        "\n",
        "\n",
        "*   Datos: https://drive.google.com/drive/folders/1diCrwQVVpXQXNV7iI1CopMPTNvW2kJVv?usp=sharing\n",
        "*   Modelo de caracteres: https://drive.google.com/drive/folders/1im_mhLNttX47cgULwt14kUh9BfoMoTGj?usp=sharing\n",
        "\n",
        "*   Embeddings: https://drive.google.com/drive/folders/1o3gRa51E76ZM8DaSW1XZSYE1gJNa6yu2?usp=sharing\n",
        "*   Modelo con embeddings: https://drive.google.com/drive/folders/1-0Dmtlyjr6TTN6VkcEOYymy6wZA8or7A?usp=sharing\n",
        "*   Modelo de Bengio: https://drive.google.com/drive/folders/1csiQZspCRISK0yOIJrmsMY2thk-dZbto?usp=sharing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ziyjtgzeVHd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Realizar modelo de lenguaje basado en caracteres con ngramas de tamaño 6"
      ],
      "metadata": {
        "id": "jlY-3RdFVWKa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EWu1Knx0NHxO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import random\n",
        "from typing import Tuple\n",
        "from argparse import Namespace\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk import FreqDist\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 1111\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "4VO4KaEune-8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/mex_data/mex_train.txt',sep = '\\r\\n',engine = 'python', header = None).loc[:,0].values.tolist()\n",
        "X_val = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/mex_data/mex_val.txt',sep = '\\r\\n',engine = 'python', header = None).loc[:,0].values.tolist()\n",
        "display(X_train[0],X_val[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Baw2AUf1pasd",
        "outputId": "fec5ff7c-337c-4db6-89de-f19cd9ff7950"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'lo peor de todo es que no me dan por un tiempo y luego vuelven estoy hasta la verga de estl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'así debería ser siempre para que se mueran a la verga'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = Namespace()\n",
        "args.N = 6"
      ],
      "metadata": {
        "id": "6k6cUAogqxzU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### El tokenizador default se ajustó a una función que regresa la lista de texto separada en caracteres."
      ],
      "metadata": {
        "id": "6-THEDwpVgL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NgramData():\n",
        "\n",
        "  def __init__(self,N: int,vocab_max: int = 5000, tokenizer = None, embeddings_model = None):\n",
        "    self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
        "    self.punct = set(['@usuario','<url>','@','*','<','>','\"','?','¿','!','¡','»','^','-',':',';','{','}','\\xa0','\\u200d','«',\"¬\",\"°\"])\n",
        "    self.N = N\n",
        "    self.vocab_max = vocab_max\n",
        "    self.UNK = \"<unk>\"\n",
        "    self.SOS = '<s>'\n",
        "    self.EOS = \"</s>\"\n",
        "    self.embeddings_model = embeddings_model\n",
        "    if embeddings_model is not None:\n",
        "      self.embeddings_model_vsize = len(embeddings_model[list(embeddings_model.keys())[0]])\n",
        "  \n",
        "  def get_vocab_size(self) -> int:\n",
        "    return len(self.vocab)\n",
        "\n",
        "  def default_tokenizer(self, doc:str) -> list:\n",
        "    return [*doc]\n",
        "  \n",
        "  def remove_word(self, word: str) -> bool:\n",
        "    word = word.lower()\n",
        "    is_punct = True if word in self.punct else False\n",
        "    is_digit = word.isnumeric()\n",
        "    return is_punct or is_digit\n",
        "\n",
        "  def get_vocab(self, corpus: list) -> set:\n",
        "    freq_dist = FreqDist([w.lower() for sentence in corpus\\\n",
        "                          for w in self.tokenizer(sentence)\\\n",
        "                          if not self.remove_word(w)])\n",
        "\n",
        "    sorted_words = self.sortFreqDict(freq_dist)[:self.vocab_max-3]\n",
        "    return set(sorted_words)\n",
        "  \n",
        "  def sortFreqDict(self, freq_dist) -> list:\n",
        "    freq_dict = dict(freq_dist)\n",
        "    return sorted(freq_dict, key = freq_dict.get, reverse = True)\n",
        "\n",
        "  def fit(self, corpus: list) -> None:\n",
        "    self.vocab = self.get_vocab(corpus)\n",
        "    self.vocab.add(self.UNK)\n",
        "    self.vocab.add(self.SOS)\n",
        "    self.vocab.add(self.EOS)\n",
        "    self.w2id = {}\n",
        "    self.id2w = {}\n",
        "\n",
        "    if self.embeddings_model is not None:\n",
        "      self.embedding_matrix = np.empty([len(self.vocab), self.embeddings_model_vsize])\n",
        "    \n",
        "    id = 0\n",
        "    for doc in corpus:\n",
        "      for word in self.tokenizer(doc):\n",
        "        word_ = word.lower()\n",
        "        if word_ in self.vocab and not word_ in self.w2id:\n",
        "          self.w2id[word_] = id\n",
        "          self.id2w[id] = word_\n",
        "          \n",
        "          if self.embeddings_model is not None:\n",
        "            if word_ in self.embeddings_model:\n",
        "              self.embedding_matrix[id] = self.embeddings_model[word_]\n",
        "            else:\n",
        "              self.embedding_matrix[id] = np.random.rand(self.embeddings_model_vsize)\n",
        "          \n",
        "          id+=1\n",
        "    \n",
        "    self.w2id.update({self.UNK: id, self.SOS: id+1, self.EOS: id+2})\n",
        "    self.id2w.update({id: self.UNK, id+1: self.SOS, id+2: self.EOS})\n",
        "\n",
        "  \n",
        "  def transform(self, corpus: list) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    X_ngrams = []\n",
        "    y = []\n",
        "\n",
        "    for doc in corpus:\n",
        "      doc_ngram = self.get_ngram_doc(doc)\n",
        "      for words_window in doc_ngram:\n",
        "        words_window_ids = [self.w2id[w] for w in words_window]\n",
        "        X_ngrams.append(list(words_window_ids[:-1]))\n",
        "        y.append(words_window_ids[-1])\n",
        "    \n",
        "    return np.array(X_ngrams), np.array(y)\n",
        "  \n",
        "  def get_ngram_doc(self, doc: str) -> list:\n",
        "    doc_tokens = self.tokenizer(doc)\n",
        "    doc_tokens = self.replace_unk(doc_tokens)\n",
        "    doc_tokens = [w.lower() for w in doc_tokens]\n",
        "    doc_tokens = [self.SOS]*(self.N-1) + doc_tokens + [self.EOS]\n",
        "    return list(ngrams(doc_tokens, self.N))\n",
        "  \n",
        "  def replace_unk(self, doc_tokens: list) -> list:\n",
        "    for i, token in enumerate(doc_tokens):\n",
        "      if token.lower() not in self.vocab:\n",
        "        doc_tokens[i] = self.UNK\n",
        "    \n",
        "    return doc_tokens"
      ],
      "metadata": {
        "id": "VG55K8WIrwls"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk = TweetTokenizer()\n",
        "ngram_data = NgramData(args.N,300)#,tk.tokenize)\n",
        "ngram_data.fit(X_train)"
      ],
      "metadata": {
        "id": "jewP370jrbB8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
        "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
      ],
      "metadata": {
        "id": "onRNJ212rhpR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training observations: {X_ngram_train.shape}, y: {y_ngram_train.shape}')\n",
        "print(f'Validation observations: {X_ngram_val.shape}, y: {y_ngram_val.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69fG4KR_BRBE",
        "outputId": "405f6384-5e2a-44f5-be0c-753016390f03"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training observations: (498949, 5), y: (498949,)\n",
            "Validation observations: (54110, 5), y: (54110,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[[ngram_data.id2w[w] for w in tw] for tw in X_ngram_train[:9]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMkdlqFXH_9v",
        "outputId": "d3384775-51d9-44a5-c983-76318607d96b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<s>', '<s>', '<s>', '<s>', '<s>'],\n",
              " ['<s>', '<s>', '<s>', '<s>', 'l'],\n",
              " ['<s>', '<s>', '<s>', 'l', 'o'],\n",
              " ['<s>', '<s>', 'l', 'o', ' '],\n",
              " ['<s>', 'l', 'o', ' ', 'p'],\n",
              " ['l', 'o', ' ', 'p', 'e'],\n",
              " ['o', ' ', 'p', 'e', 'o'],\n",
              " [' ', 'p', 'e', 'o', 'r'],\n",
              " ['p', 'e', 'o', 'r', ' ']]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[ngram_data.id2w[w] for w  in y_ngram_train[:9]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDPx_Y84JCu3",
        "outputId": "722e6fd7-8fce-4479-bf6c-87c79d49eb0f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['l', 'o', ' ', 'p', 'e', 'o', 'r', ' ', 'd']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args.batch_size = 64\n",
        "args.num_workers = 2\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype = torch.int64),\n",
        "                              torch.tensor(y_ngram_train, dtype = torch.int64))\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=args.batch_size,\n",
        "                          num_workers=args.num_workers,\n",
        "                          shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype = torch.int64),\n",
        "                              torch.tensor(y_ngram_val, dtype = torch.int64))\n",
        "\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                          batch_size=args.batch_size,\n",
        "                          num_workers=args.num_workers,\n",
        "                          shuffle=False)"
      ],
      "metadata": {
        "id": "fJalvrw6JOyX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))\n",
        "print(f'X shape: {batch[0].shape}')\n",
        "print(f'y shape: {batch[1].shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL8PY4t6K9RR",
        "outputId": "0ffa5a09-b122-4017-a51e-5786e3c8211b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: torch.Size([64, 5])\n",
            "y shape: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralLM(nn.Module):\n",
        "\n",
        "  def __init__(self, args, embedding_matrix=None):\n",
        "    super(NeuralLM, self).__init__()\n",
        "\n",
        "    self.window_size = args.N-1\n",
        "    self.embedding_dim = args.d\n",
        "\n",
        "    self.emb = nn.Embedding(args.vocab_size, args.d)\n",
        "    if embedding_matrix is not None:\n",
        "      dtp = self.emb.weight.dtype\n",
        "      self.emb.weight = torch.nn.Parameter(torch.from_numpy(embedding_matrix).to(dtp))\n",
        "    self.fc1 = nn.Linear(args.d*(args.N-1), args.d_h)\n",
        "    self.drop1 = nn.Dropout(p=args.dropout)\n",
        "    self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.emb(x)\n",
        "    x = x.view(-1, self.window_size*self.embedding_dim)\n",
        "    h = F.relu(self.fc1(x))\n",
        "    h = self.drop1(h)\n",
        "    return self.fc2(h)"
      ],
      "metadata": {
        "id": "1C8jp0mWL-Fw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_preds(raw_logits):\n",
        "  probs = F.softmax(raw_logits.detach(), dim=1)\n",
        "  y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "\n",
        "  return y_pred\n",
        "\n",
        "def model_eval(data, model, gpu=False):\n",
        "  with torch.no_grad():\n",
        "    preds, tgts = [],[]\n",
        "    for window_words,labels in data:\n",
        "      if gpu:\n",
        "        window_words = window_words.cuda()\n",
        "      \n",
        "      outputs = model(window_words)\n",
        "\n",
        "      #predictions\n",
        "      y_pred = get_preds(outputs)\n",
        "      tgt = labels.numpy()\n",
        "      tgts.append(tgt)\n",
        "      preds.append(y_pred)\n",
        "\n",
        "  tgts = [e for l in tgts for e in l]\n",
        "  preds = [e for l in preds for e in l]\n",
        "  return accuracy_score(tgts, preds)\n",
        "\n",
        "def save_checkpoint(state, is_best, checkpoint_path, filename = \"checkpoint.pt\"):\n",
        "  filename = os.path.join(checkpoint_path, filename)\n",
        "  torch.save(state, filename)\n",
        "  if is_best:\n",
        "    shutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))"
      ],
      "metadata": {
        "id": "nru5Zn2zNihr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "args.vocab_size = ngram_data.get_vocab_size()\n",
        "args.d = 128        #dimensión de word embeddings\n",
        "args.d_h = 256      #HL\n",
        "args.dropout = 0.1\n",
        "\n",
        "#Training parameters\n",
        "args.lr = 2.3e-1\n",
        "args.num_epochs = 100\n",
        "args.patience = 20\n",
        "\n",
        "# Scheduler hyperparameters\n",
        "args.lr_patience = 10\n",
        "args.lr_factor = 0.5\n",
        "\n",
        "#saving directory\n",
        "args.savedir = '/content/drive/MyDrive/Colab Notebooks/model_char'\n",
        "os.makedirs(args.savedir, exist_ok=True)\n",
        "\n",
        "#Create model\n",
        "model = NeuralLM(args)\n",
        "\n",
        "#Send to GPU\n",
        "args.use_gpu = torch.cuda.is_available()\n",
        "if args.use_gpu:\n",
        "  model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\"min\",\n",
        "    patience=args.lr_patience,\n",
        "    verbose=True,\n",
        "    factor=args.lr_factor\n",
        ")"
      ],
      "metadata": {
        "id": "ix9u7EQ8RypQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "best_metric = 0\n",
        "metric_history = []\n",
        "train_metric_history = []\n",
        "\n",
        "for epoch in range(args.num_epochs):\n",
        "  epoch_start_time = time.time()\n",
        "  loss_epoch = []\n",
        "  training_metric = []\n",
        "  model.train()\n",
        "\n",
        "  for window_words, labels in train_loader:\n",
        "\n",
        "    if args.use_gpu:\n",
        "      window_words = window_words.cuda()\n",
        "      labels = labels.cuda()\n",
        "\n",
        "    outputs = model(window_words)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss_epoch.append(loss.item())\n",
        "\n",
        "    y_pred = get_preds(outputs)\n",
        "    tgt = labels.cpu().numpy()\n",
        "    training_metric.append(accuracy_score(tgt, y_pred))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  mean_epoch_metric = np.mean(training_metric)\n",
        "  train_metric_history.append(mean_epoch_metric)\n",
        "\n",
        "  model.eval()\n",
        "  tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
        "  metric_history.append(mean_epoch_metric)\n",
        "\n",
        "  scheduler.step(tuning_metric)\n",
        "\n",
        "  is_improvement = tuning_metric > best_metric\n",
        "  if is_improvement:\n",
        "    best_metric = tuning_metric\n",
        "    n_no_improve = 0\n",
        "\n",
        "  else:\n",
        "    n_no_improve += 1\n",
        "  \n",
        "  save_checkpoint(\n",
        "      {\n",
        "          \"epoch\": epoch + 1,\n",
        "          \"state_dict\": model.state_dict(),\n",
        "          \"optimizer\": optimizer.state_dict(),\n",
        "          \"scheduler\": scheduler.state_dict(),\n",
        "          \"best_metric\": best_metric,\n",
        "      },\n",
        "      is_improvement,\n",
        "      args.savedir,\n",
        "  )\n",
        "\n",
        "  if n_no_improve >= args.patience:\n",
        "    print(\"No improvement. Breaking out of loop.\")\n",
        "    break\n",
        "\n",
        "  print('Train acc: {}'.format(mean_epoch_metric))\n",
        "  print('Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}'\n",
        "  .format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
        "\n",
        "print(\"--- %s seconds ---\" %(time.time() - start_time))"
      ],
      "metadata": {
        "id": "leylPXRLTwel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = NeuralLM(args)\n",
        "best_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/model_char/model_best.pt')['state_dict'])\n",
        "best_model.train(False);"
      ],
      "metadata": {
        "id": "zRk4hzSBlDY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenize\n",
        "def parse_text(text, tokenizer):\n",
        "  #all_tokens = [w.lower() if w in ngram_data.w2id else '<unk>' for w in tokenizer.tokenize(text)]\n",
        "  all_tokens = [w.lower() if w in ngram_data.w2id else '<unk>' for w in [*text]]\n",
        "  token_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
        "  return all_tokens, token_ids"
      ],
      "metadata": {
        "id": "JxbdudWbv9fX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_next_word(logits, temperature = 1.0):\n",
        "  logits = np.asarray(logits).astype('float64')\n",
        "  preds = logits/temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds/np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1,preds)\n",
        "  return np.argmax(probas)\n",
        "\n",
        "def predict_next_token(model, tokens_ids):\n",
        "  word_ids_tensor = torch.LongTensor(tokens_ids).unsqueeze(0)\n",
        "  y_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
        "  y_pred = sample_next_word(y_raw_pred, 1.0)\n",
        "  return y_pred\n",
        "\n",
        "def generate_sentence(model, initial_text, tokenizer):\n",
        "  all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
        "  for i in range(300):\n",
        "    y_pred = predict_next_token(best_model, window_word_ids)\n",
        "    next_word = ngram_data.id2w[y_pred]\n",
        "    all_tokens.append(next_word)\n",
        "    if next_word == '</s>':\n",
        "      break\n",
        "    else:\n",
        "      window_word_ids.pop(0)\n",
        "      window_word_ids.append(y_pred)\n",
        "  return \"\".join(all_tokens)"
      ],
      "metadata": {
        "id": "B3n3x-DJoa3R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargamos el mejor modelo entrenado y realizamos las pruebas solicitadas. (Borré la salida del entrenamiento previo por accidente, no quise volverla a correr porque tarda mucho en mi cpu)"
      ],
      "metadata": {
        "id": "7hSjRaYPV7Cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de texto:"
      ],
      "metadata": {
        "id": "0jfuPwSkWUXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_tokens = \"me gu\"\n",
        "print(generate_sentence(best_model, initial_tokens, tk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a3hTmbipEUY",
        "outputId": "6fa059e1-f1b0-429e-deef-84fddffca40c"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "me gustas la propedo es que les huebrazos... soy putos de corriet de #felizmiercole vos al vikienar al mandar <unk><unk> es argentincumente por feuso que tienen talle dejarles a mamá estas pero si cambio chingas aconteste cualquiero el mismo quiera mamando. <unk>)</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_tokens = \"no cr\"\n",
        "print(generate_sentence(best_model, initial_tokens, tk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOjBQPYYqE0L",
        "outputId": "f1453192-d901-4de3-a1a0-f53c46a753b3"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no cres lo va a faltan notrat de sus hdp hahahh perros</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_tokens = \"soy e\"\n",
        "print(generate_sentence(best_model, initial_tokens, tk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxlKzieeqPWK",
        "outputId": "b148458b-d670-475d-cbc6-c50908028cba"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "soy ese en los metodeli te los persona y otra de sería los juegos .️</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Medición de loglikelihood"
      ],
      "metadata": {
        "id": "1R5eZqQYWZAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_likelihood(model, text, ngram_model):\n",
        "  X,y = ngram_data.transform([text])\n",
        "  X,y = X[2:],y[2:]\n",
        "  X = torch.LongTensor(X).unsqueeze(0)\n",
        "  logits = model(X).detach()\n",
        "  probs = F.softmax(logits, dim = 1).numpy()\n",
        "  return np.sum([np.log(probs[i][w]) for i,w in enumerate(y)])"
      ],
      "metadata": {
        "id": "3TB_KHhdooAQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frases = [\"leo a hegel pero soy materialista\",\"sí estoy de acuerdo compañero \",\n",
        "          \"koolksdjk hb oslskdxxjflskdjf \",\n",
        "          \"chinguen a sus madres cabrones \",\"dios es mi pastor, nada me falta\"]\n",
        "for ph in frases:\n",
        "  print(\"log likelihood de '\"+ph+\"': \", log_likelihood(best_model, ph,ngram_data))"
      ],
      "metadata": {
        "id": "767ERQa2qcE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9827218c-269d-49f2-ae79-4282b45d0e53"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood de 'leo a hegel pero soy materialista':  -64.0448\n",
            "log likelihood de 'sí estoy de acuerdo compañero ':  -43.65916\n",
            "log likelihood de 'koolksdjk hb oslskdxxjflskdjf ':  -221.7628\n",
            "log likelihood de 'chinguen a sus madres cabrones ':  -37.46643\n",
            "log likelihood de 'dios es mi pastor, nada me falta':  -54.523594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estructuras Sintácticas"
      ],
      "metadata": {
        "id": "P0ECMdWeptRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import permutations\n",
        "from random import shuffle\n",
        "\n",
        "word_list = list(\"chingado\")\n",
        "perm = [''.join(perm) for perm in permutations(word_list)]\n",
        "\n",
        "for p, t in sorted([(log_likelihood(best_model,text,ngram_data),text) for text in perm], reverse=True)[:5]:\n",
        "  print(p,''.join(t))\n",
        "print('-'*50)\n",
        "for p, t in sorted([(log_likelihood(best_model,text,ngram_data),text) for text in perm], reverse=True)[-5:]:\n",
        "  print(p,''.join(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnQWf2sfpriA",
        "outputId": "42c70a58-fbc7-40f2-e535-d8a56163999b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-8.226238 chingado\n",
            "-12.024684 ghicando\n",
            "-12.834872 dgichona\n",
            "-12.950214 chingoda\n",
            "-13.811747 hgnicado\n",
            "--------------------------------------------------\n",
            "-80.1418 cahgdnoi\n",
            "-80.60216 iadhnocg\n",
            "-80.69323 hdoaicng\n",
            "-83.08769 haocgdni\n",
            "-83.404396 nahgdcoi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Medición de perplejidad sobre los datos de validación."
      ],
      "metadata": {
        "id": "qKQlljiVWkIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Prpx(N,p):\n",
        "  return(np.power(2,-(1/N)*p))"
      ],
      "metadata": {
        "id": "NKtZsP0GHgi2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ' '.join(X_val)\n",
        "Prpx(len(text),log_likelihood(best_model,text,ngram_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppv0Pa6CHng5",
        "outputId": "eebd31d0-9838-4808-f0e2-039fb7a13f5a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.10855247056122"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Realizamos el mismo modelo que en la práctica pero cargando los embeddings proporcionados word2vec con ngramas de tamaño 3."
      ],
      "metadata": {
        "id": "ZbD1WyyOlkAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/word2vec/word2vec_col.txt','r') as f:\n",
        "  embw = [line.split() for line in f]\n",
        "embd = {}\n",
        "for i in range(1,len(embw)): embd[embw[i][0]] = [float(x) for x in embw[i][1:]]"
      ],
      "metadata": {
        "id": "LIP9a0ajbIiY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk = TweetTokenizer()\n",
        "args = Namespace()\n",
        "args.N = 4\n",
        "ngram_data = NgramData(args.N,5000,tk.tokenize,embeddings_model=embd)\n",
        "ngram_data.fit(X_train)\n",
        "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
        "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)\n",
        "del embd"
      ],
      "metadata": {
        "id": "W9fh4ZvNc51P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.batch_size = 64\n",
        "args.num_workers = 2\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype = torch.int64),\n",
        "                              torch.tensor(y_ngram_train, dtype = torch.int64))\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=args.batch_size,\n",
        "                          num_workers=args.num_workers,\n",
        "                          shuffle=True)\n",
        "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype = torch.int64),\n",
        "                              torch.tensor(y_ngram_val, dtype = torch.int64))\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                          batch_size=args.batch_size,\n",
        "                          num_workers=args.num_workers,\n",
        "                          shuffle=False)"
      ],
      "metadata": {
        "id": "9yqYuYA7d2Pg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.vocab_size = ngram_data.get_vocab_size()\n",
        "args.d = 100\n",
        "args.d_h = 256\n",
        "args.dropout = 0.1\n",
        "args.lr = 1e-2\n",
        "args.num_epochs = 200\n",
        "args.patience = 20\n",
        "args.lr_patience = 10\n",
        "args.lr_factor = 0.5\n",
        "\n",
        "args.savedir = '/content/drive/MyDrive/Colab Notebooks/nmodel'\n",
        "os.makedirs(args.savedir, exist_ok=True)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "CHKtQM9_d5iK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralLM(args,ngram_data.embedding_matrix)\n",
        "args.use_gpu = torch.cuda.is_available()\n",
        "if args.use_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\"min\",\n",
        "    patience=args.lr_patience,\n",
        "    verbose=True,\n",
        "    factor=args.lr_factor\n",
        ")"
      ],
      "metadata": {
        "id": "3BGMQg4XePeY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "best_metric = 0\n",
        "metric_history = []\n",
        "train_metric_history = []\n",
        "\n",
        "for epoch in range(args.num_epochs):\n",
        "  epoch_start_time = time.time()\n",
        "  loss_epoch = []\n",
        "  training_metric = []\n",
        "  model.train()\n",
        "\n",
        "  for window_words, labels in train_loader:\n",
        "\n",
        "    if args.use_gpu:\n",
        "      window_words = window_words.cuda()\n",
        "      labels = labels.cuda()\n",
        "\n",
        "    outputs = model(window_words)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss_epoch.append(loss.item())\n",
        "\n",
        "    y_pred = get_preds(outputs)\n",
        "    tgt = labels.cpu().numpy()\n",
        "    training_metric.append(accuracy_score(tgt, y_pred))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  mean_epoch_metric = np.mean(training_metric)\n",
        "  train_metric_history.append(mean_epoch_metric)\n",
        "\n",
        "  model.eval()\n",
        "  tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
        "  metric_history.append(mean_epoch_metric)\n",
        "\n",
        "  scheduler.step(tuning_metric)\n",
        "\n",
        "  is_improvement = tuning_metric > best_metric\n",
        "  if is_improvement:\n",
        "    best_metric = tuning_metric\n",
        "    n_no_improve = 0\n",
        "\n",
        "  else:\n",
        "    n_no_improve += 1\n",
        "  \n",
        "  save_checkpoint(\n",
        "      {\n",
        "          \"epoch\": epoch + 1,\n",
        "          \"state_dict\": model.state_dict(),\n",
        "          \"optimizer\": optimizer.state_dict(),\n",
        "          \"scheduler\": scheduler.state_dict(),\n",
        "          \"best_metric\": best_metric,\n",
        "      },\n",
        "      is_improvement,\n",
        "      args.savedir,\n",
        "  )\n",
        "\n",
        "  if n_no_improve >= args.patience:\n",
        "    print(\"No improvement. Breaking out of loop.\")\n",
        "    break\n",
        "\n",
        "  print('Train acc: {}'.format(mean_epoch_metric))\n",
        "  print('Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}'\n",
        "  .format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
        "\n",
        "print(\"--- %s seconds ---\" %(time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5drDg0xfzKm",
        "outputId": "d0876a91-576e-44a4-ead0-ee8f8f8a9121"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc: 0.1651429110399917\n",
            "Epoch [1/200], Loss: 5.6515 - Val accuracy: 0.2110 - Epoch time: 43.01\n",
            "Train acc: 0.181641997111857\n",
            "Epoch [2/200], Loss: 5.3284 - Val accuracy: 0.2110 - Epoch time: 45.02\n",
            "Train acc: 0.1879236268311087\n",
            "Epoch [3/200], Loss: 5.1417 - Val accuracy: 0.2133 - Epoch time: 42.67\n",
            "Train acc: 0.1920306117138917\n",
            "Epoch [4/200], Loss: 5.0335 - Val accuracy: 0.2163 - Epoch time: 41.58\n",
            "Train acc: 0.19497241953529518\n",
            "Epoch [5/200], Loss: 4.9534 - Val accuracy: 0.2200 - Epoch time: 41.91\n",
            "Train acc: 0.1969189888897562\n",
            "Epoch [6/200], Loss: 4.8874 - Val accuracy: 0.2224 - Epoch time: 41.54\n",
            "Train acc: 0.1991859209533474\n",
            "Epoch [7/200], Loss: 4.8296 - Val accuracy: 0.2199 - Epoch time: 41.32\n",
            "Train acc: 0.2004202117971535\n",
            "Epoch [8/200], Loss: 4.7799 - Val accuracy: 0.2239 - Epoch time: 42.24\n",
            "Train acc: 0.20347219511877812\n",
            "Epoch [9/200], Loss: 4.7319 - Val accuracy: 0.2236 - Epoch time: 45.24\n",
            "Train acc: 0.20468006010459763\n",
            "Epoch [10/200], Loss: 4.6913 - Val accuracy: 0.2227 - Epoch time: 42.93\n",
            "Train acc: 0.2060733126479848\n",
            "Epoch [11/200], Loss: 4.6495 - Val accuracy: 0.2204 - Epoch time: 42.71\n",
            "Epoch 00012: reducing learning rate of group 0 to 5.0000e-03.\n",
            "Train acc: 0.20766130343715036\n",
            "Epoch [12/200], Loss: 4.6115 - Val accuracy: 0.2255 - Epoch time: 42.32\n",
            "Train acc: 0.21136783493352068\n",
            "Epoch [13/200], Loss: 4.5558 - Val accuracy: 0.2222 - Epoch time: 42.46\n",
            "Train acc: 0.213336764629355\n",
            "Epoch [14/200], Loss: 4.5346 - Val accuracy: 0.2252 - Epoch time: 44.64\n",
            "Train acc: 0.21455276064840112\n",
            "Epoch [15/200], Loss: 4.5163 - Val accuracy: 0.2248 - Epoch time: 42.62\n",
            "Train acc: 0.214831248536414\n",
            "Epoch [16/200], Loss: 4.4996 - Val accuracy: 0.2182 - Epoch time: 42.62\n",
            "Train acc: 0.2148780019774673\n",
            "Epoch [17/200], Loss: 4.4822 - Val accuracy: 0.2250 - Epoch time: 42.13\n",
            "Train acc: 0.215285773293784\n",
            "Epoch [18/200], Loss: 4.4648 - Val accuracy: 0.2234 - Epoch time: 42.25\n",
            "Train acc: 0.2157175311581193\n",
            "Epoch [19/200], Loss: 4.4451 - Val accuracy: 0.2278 - Epoch time: 42.37\n",
            "Train acc: 0.21608464730830274\n",
            "Epoch [20/200], Loss: 4.4299 - Val accuracy: 0.2271 - Epoch time: 43.44\n",
            "Train acc: 0.2173705702130981\n",
            "Epoch [21/200], Loss: 4.4152 - Val accuracy: 0.2282 - Epoch time: 42.94\n",
            "Train acc: 0.21774663049983087\n",
            "Epoch [22/200], Loss: 4.3988 - Val accuracy: 0.2228 - Epoch time: 42.81\n",
            "Epoch 00023: reducing learning rate of group 0 to 2.5000e-03.\n",
            "Train acc: 0.2191118309785861\n",
            "Epoch [23/200], Loss: 4.3796 - Val accuracy: 0.2247 - Epoch time: 42.93\n",
            "Train acc: 0.22056322040954388\n",
            "Epoch [24/200], Loss: 4.3535 - Val accuracy: 0.2252 - Epoch time: 42.95\n",
            "Train acc: 0.22260329664611142\n",
            "Epoch [25/200], Loss: 4.3440 - Val accuracy: 0.2271 - Epoch time: 43.18\n",
            "Train acc: 0.22216218809356542\n",
            "Epoch [26/200], Loss: 4.3359 - Val accuracy: 0.2246 - Epoch time: 43.50\n",
            "Train acc: 0.22308993898472665\n",
            "Epoch [27/200], Loss: 4.3276 - Val accuracy: 0.2244 - Epoch time: 43.57\n",
            "Train acc: 0.2229175610803216\n",
            "Epoch [28/200], Loss: 4.3182 - Val accuracy: 0.2252 - Epoch time: 42.96\n",
            "Train acc: 0.22324117620274245\n",
            "Epoch [29/200], Loss: 4.3127 - Val accuracy: 0.2246 - Epoch time: 42.06\n",
            "Train acc: 0.22333508963651028\n",
            "Epoch [30/200], Loss: 4.3041 - Val accuracy: 0.2250 - Epoch time: 43.24\n",
            "Train acc: 0.22231911703483984\n",
            "Epoch [31/200], Loss: 4.2980 - Val accuracy: 0.2271 - Epoch time: 43.76\n",
            "Train acc: 0.22322654034293446\n",
            "Epoch [32/200], Loss: 4.2885 - Val accuracy: 0.2257 - Epoch time: 43.57\n",
            "Train acc: 0.22426406018265552\n",
            "Epoch [33/200], Loss: 4.2802 - Val accuracy: 0.2248 - Epoch time: 42.13\n",
            "Epoch 00034: reducing learning rate of group 0 to 1.2500e-03.\n",
            "Train acc: 0.2239404450602347\n",
            "Epoch [34/200], Loss: 4.2728 - Val accuracy: 0.2262 - Epoch time: 41.69\n",
            "Train acc: 0.22481209182213202\n",
            "Epoch [35/200], Loss: 4.2571 - Val accuracy: 0.2268 - Epoch time: 42.32\n",
            "Train acc: 0.22524222347982203\n",
            "Epoch [36/200], Loss: 4.2541 - Val accuracy: 0.2271 - Epoch time: 42.41\n",
            "Train acc: 0.22600166198319155\n",
            "Epoch [37/200], Loss: 4.2472 - Val accuracy: 0.2272 - Epoch time: 41.96\n",
            "Train acc: 0.2252962948507793\n",
            "Epoch [38/200], Loss: 4.2451 - Val accuracy: 0.2262 - Epoch time: 41.85\n",
            "Train acc: 0.2257524458147946\n",
            "Epoch [39/200], Loss: 4.2434 - Val accuracy: 0.2249 - Epoch time: 41.89\n",
            "Train acc: 0.2259549085421383\n",
            "Epoch [40/200], Loss: 4.2369 - Val accuracy: 0.2260 - Epoch time: 41.96\n",
            "No improvement. Breaking out of loop.\n",
            "--- 1751.286489725113 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = NeuralLM(args)\n",
        "best_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/nmodel/model_best.pt')['state_dict'])\n",
        "best_model.train(False);"
      ],
      "metadata": {
        "id": "xPallUg5nh7n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenize\n",
        "def parse_text(text, tokenizer):\n",
        "  all_tokens = [w.lower() if w in ngram_data.w2id else '<unk>' for w in tokenizer.tokenize(text)]\n",
        "  token_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
        "  return all_tokens, token_ids\n",
        "\n",
        "def sample_next_word(logits, temperature = 1.0):\n",
        "  logits = np.asarray(logits).astype('float64')\n",
        "  preds = logits/temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds/np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1,preds)\n",
        "  return np.argmax(probas)\n",
        "\n",
        "def predict_next_token(model, tokens_ids):\n",
        "  word_ids_tensor = torch.LongTensor(tokens_ids).unsqueeze(0)\n",
        "  y_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
        "  y_pred = sample_next_word(y_raw_pred, 1.0)\n",
        "  return y_pred\n",
        "\n",
        "def generate_sentence(model, initial_text, tokenizer):\n",
        "  all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
        "  for i in range(100):\n",
        "    y_pred = predict_next_token(best_model, window_word_ids)\n",
        "    next_word = ngram_data.id2w[y_pred]\n",
        "    all_tokens.append(next_word)\n",
        "\n",
        "    if next_word == '</s>':\n",
        "      break\n",
        "    else:\n",
        "      window_word_ids.pop(0)\n",
        "      window_word_ids.append(y_pred)\n",
        "  return \" \".join(all_tokens)"
      ],
      "metadata": {
        "id": "AvjruAXPnqJo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargamos el mejor modelo y realizamos las pruebas solicitadas."
      ],
      "metadata": {
        "id": "FbI90moEXBs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Palabras más cercanas usando los embeddings cargados."
      ],
      "metadata": {
        "id": "h3CxRRgvXG1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_closest_words(embeddings, ngram_data, word, n):\n",
        "  word_id = torch.LongTensor([ngram_data.w2id[word]]) #get word id\n",
        "  word_embed = embeddings(word_id) #get word embedding\n",
        "  dists = torch.norm(embeddings.weight - word_embed, dim=1).detach()\n",
        "  lst = sorted(enumerate(dists.numpy()), key = lambda x: x[1]) #sort by distance\n",
        "  for idx, difference in lst[1:n+1]:\n",
        "    print(ngram_data.id2w[idx],difference)"
      ],
      "metadata": {
        "id": "4hkXEWoEmUlp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_closest_words(best_model.emb, ngram_data, \"perro\", 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLsdR-STmj9-",
        "outputId": "d5ad0428-efe5-4b97-e218-2fd6de27adbb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gato 9.554484\n",
            "perrito 12.496251\n",
            "gatito 15.656715\n",
            "enano 17.228447\n",
            "gordo 17.415058\n",
            "vecino 17.43069\n",
            "niño 17.560242\n",
            "macho 17.807663\n",
            "sapo 17.827457\n",
            "mono 17.899689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_closest_words(best_model.emb, ngram_data, \"pendejo\", 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVKqhywumtDB",
        "outputId": "0ec7e0e1-c682-4625-f53a-f1cf4a94d759"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imbécil 9.731764\n",
            "tonto 11.607184\n",
            "idiota 11.913824\n",
            "tarado 12.543361\n",
            "cabrón 13.262273\n",
            "maricon 13.555503\n",
            "marica 13.810448\n",
            "mamón 13.912109\n",
            "maricón 14.1301985\n",
            "cabron 14.339897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_closest_words(best_model.emb, ngram_data, \"youtuber\", 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTFJavcQm4nn",
        "outputId": "16a9b3d3-a180-4f82-f5d3-31cbc5f6087e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tuitero 13.398776\n",
            "pendejazo 15.068455\n",
            "puberto 15.095328\n",
            "dalas 15.3225565\n",
            "profesionista 15.408114\n",
            "meme 15.463341\n",
            "jotito 15.641256\n",
            "reportero 15.663945\n",
            "fake 15.705067\n",
            "mayate 15.72003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de texto"
      ],
      "metadata": {
        "id": "vvcg3XnpYMSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_tokens = \"<s> hola amigo\"\n",
        "print(generate_sentence(best_model, initial_tokens, tk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wrp2Orkgn22O",
        "outputId": "e56afcac-ef9b-430b-b190-f75f9dce9588"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> hola amigo tiene mil putas doble <unk> <unk> pedas v a todos a supe que nos vale . </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_tokens = \"soy una persona\"\n",
        "print(generate_sentence(best_model, initial_tokens, tk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxBKkO42PRKm",
        "outputId": "98fecefa-296f-4ca6-a597-b9a0f2f89f32"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "soy una persona mejor día del mejor ahora es <unk> pero todo <unk> lo que #putita porque era playera de mujeres no se vuelva a mundial y todos los <unk> 😻 yo gay si no sabe ( qué le caga el <unk> kilos de su puta madre </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_tokens = \"yo creo que\"\n",
        "print(generate_sentence(best_model, initial_tokens, tk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_k9LhnAPc9d",
        "outputId": "3c0a5c07-6590-49b6-aaaf-8927bdef4a2a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yo creo que quiero escuchar perder el día de cuenta porque dos los demás a la 💕 saliste <unk> <unk> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Medición de loglikelihood"
      ],
      "metadata": {
        "id": "SCzPuxsDYPU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frases = [\"leo a hegel pero soy materialista\",\"sí estoy de acuerdo compañero \",\n",
        "          \"koolksdjk hb oslskdxxjflskdjf \",\n",
        "          \"chinguen a sus madres cabrones \",\"dios es mi pastor, nada me falta\"]\n",
        "for ph in frases:\n",
        "  print(\"log likelihood de '\"+ph+\"': \", log_likelihood(best_model, ph,ngram_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pEZcXi4Puz_",
        "outputId": "fc8003c3-ef1f-40d1-a7a9-fdc25fddbc27"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood de 'leo a hegel pero soy materialista':  -18.066015\n",
            "log likelihood de 'sí estoy de acuerdo compañero ':  -29.875872\n",
            "log likelihood de 'koolksdjk hb oslskdxxjflskdjf ':  -3.381772\n",
            "log likelihood de 'chinguen a sus madres cabrones ':  -24.233398\n",
            "log likelihood de 'dios es mi pastor, nada me falta':  -27.192327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estructuras sintácticas"
      ],
      "metadata": {
        "id": "7LHU_7mIYT_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import permutations\n",
        "from random import shuffle\n",
        "\n",
        "word_list = \"leo messi es campeón del mundo\".split(' ')\n",
        "perm = [' '.join(perm) for perm in permutations(word_list)]\n",
        "\n",
        "for p, t in sorted([(log_likelihood(best_model,text,ngram_data),text) for text in perm], reverse=True)[:5]:\n",
        "  print(p,t)\n",
        "print('-'*50)\n",
        "for p, t in sorted([(log_likelihood(best_model,text,ngram_data),text) for text in perm], reverse=True)[-5:]:\n",
        "  print(p,t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfsJwI75P7zD",
        "outputId": "6e73bb56-7fdd-446a-a756-30f65e4c2f5e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-24.187115 leo campeón del mundo es messi\n",
            "-25.334713 leo messi es campeón del mundo\n",
            "-25.383858 messi campeón del mundo es leo\n",
            "-25.441917 messi campeón es del mundo leo\n",
            "-25.717482 campeón messi del mundo es leo\n",
            "--------------------------------------------------\n",
            "-48.267155 del es leo campeón mundo messi\n",
            "-48.509388 messi es leo campeón mundo del\n",
            "-48.592392 messi del es leo campeón mundo\n",
            "-48.78185 del es messi campeón leo mundo\n",
            "-49.08704 es leo campeón mundo messi del\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perplejidad del modelo"
      ],
      "metadata": {
        "id": "PzEbEWF7Yx4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_likelihood(model, text, ngram_model):\n",
        "  X,y = ngram_data.transform([text])\n",
        "  n = X.shape[0]\n",
        "  X,y = X[2:],y[2:]\n",
        "  X = torch.LongTensor(X).unsqueeze(0)\n",
        "  logits = model(X).detach()\n",
        "  probs = F.softmax(logits, dim = 1).numpy()\n",
        "  return n,np.sum([np.log(probs[i][w]) for i,w in enumerate(y)])\n",
        "\n",
        "def Prpx(N,p):\n",
        "  return(np.power(2,-(1/N)*p))"
      ],
      "metadata": {
        "id": "aVD7rb7CQ9un"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N,P = 0,0\n",
        "for i in X_val:\n",
        "  n,p = log_likelihood(best_model,i,ngram_data)\n",
        "  N += n\n",
        "  P += p\n",
        "Prpx(N,P)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2iy909sRGuC",
        "outputId": "9c9b6189-56fa-4ba5-bc7e-c46b66f5ed7a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16.368275236521857"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usando los embeddings word2vec, alcanzamos una precisión de hasta 0.225 sobre los datos de entrenamiento y de casi 0.23 sobre los de validación. Mientras que la perplejidad (16.368) es menor que la calculada para el modelo realizada en clase sin embeddings (20.01)."
      ],
      "metadata": {
        "id": "GvCX46bElB4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Modelo de ngramas de tamaño 3 con conexión directa de la capa de embeddings hacía la salida."
      ],
      "metadata": {
        "id": "_vLpxDWdp6L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralLM(nn.Module):\n",
        "  def __init__(self, args, embedding_matrix=None):\n",
        "    super(NeuralLM, self).__init__()\n",
        "\n",
        "    self.window_size = args.N-1\n",
        "    self.embedding_dim = args.d\n",
        "\n",
        "    self.emb = nn.Embedding(args.vocab_size, args.d)\n",
        "    if embedding_matrix is not None:\n",
        "      dtp = self.emb.weight.dtype\n",
        "      self.emb.weight = torch.nn.Parameter(torch.from_numpy(embedding_matrix).to(dtp))\n",
        "    \n",
        "    self.fcC = nn.Linear(args.d*(args.N-1), args.vocab_size)\n",
        "    self.fc1 = nn.Linear(args.d*(args.N-1), args.d_h)\n",
        "    self.drop1 = nn.Dropout(p=args.dropout)\n",
        "    self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.emb(x)\n",
        "    x = x.view(-1, self.window_size*self.embedding_dim)\n",
        "    h = F.relu(self.fc1(x))\n",
        "    h = self.drop1(h)\n",
        "    theta = self.fcC(x)\n",
        "    return self.fc2(h)+theta"
      ],
      "metadata": {
        "id": "9Z30XcAYp72v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk = TweetTokenizer()\n",
        "args = Namespace()\n",
        "args.N = 4\n",
        "ngram_data = NgramData(args.N,5000,tk.tokenize,embeddings_model=embd)\n",
        "ngram_data.fit(X_train)\n",
        "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
        "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)\n",
        "del embd"
      ],
      "metadata": {
        "id": "twa9_Rosrgwt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.batch_size = 64\n",
        "args.num_workers = 2\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype = torch.int64),\n",
        "                              torch.tensor(y_ngram_train, dtype = torch.int64))\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=args.batch_size,\n",
        "                          num_workers=args.num_workers,\n",
        "                          shuffle=True)\n",
        "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype = torch.int64),\n",
        "                              torch.tensor(y_ngram_val, dtype = torch.int64))\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                          batch_size=args.batch_size,\n",
        "                          num_workers=args.num_workers,\n",
        "                          shuffle=False)"
      ],
      "metadata": {
        "id": "XvA5bhCsroIy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.vocab_size = ngram_data.get_vocab_size()\n",
        "args.d = 100\n",
        "args.d_h = 256\n",
        "args.dropout = 0.1\n",
        "args.lr = 1e-2\n",
        "args.num_epochs = 100\n",
        "args.patience = 20\n",
        "args.lr_patience = 10\n",
        "args.lr_factor = 0.5\n",
        "\n",
        "args.savedir = '/content/drive/MyDrive/Colab Notebooks/bengio_model'\n",
        "os.makedirs(args.savedir, exist_ok=True)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = NeuralLM(args,ngram_data.embedding_matrix)\n",
        "args.use_gpu = torch.cuda.is_available()\n",
        "if args.use_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\"min\",\n",
        "    patience=args.lr_patience,\n",
        "    verbose=True,\n",
        "    factor=args.lr_factor\n",
        ")"
      ],
      "metadata": {
        "id": "0cESGviGrs8B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "best_metric = 0\n",
        "metric_history = []\n",
        "train_metric_history = []\n",
        "\n",
        "for epoch in range(args.num_epochs):\n",
        "  epoch_start_time = time.time()\n",
        "  loss_epoch = []\n",
        "  training_metric = []\n",
        "  model.train()\n",
        "\n",
        "  for window_words, labels in train_loader:\n",
        "\n",
        "    if args.use_gpu:\n",
        "      window_words = window_words.cuda()\n",
        "      labels = labels.cuda()\n",
        "\n",
        "    outputs = model(window_words)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss_epoch.append(loss.item())\n",
        "\n",
        "    y_pred = get_preds(outputs)\n",
        "    tgt = labels.cpu().numpy()\n",
        "    training_metric.append(accuracy_score(tgt, y_pred))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  mean_epoch_metric = np.mean(training_metric)\n",
        "  train_metric_history.append(mean_epoch_metric)\n",
        "\n",
        "  model.eval()\n",
        "  tuning_metric = model_eval(val_loader, model, gpu=args.use_gpu)\n",
        "  metric_history.append(mean_epoch_metric)\n",
        "\n",
        "  scheduler.step(tuning_metric)\n",
        "\n",
        "  is_improvement = tuning_metric > best_metric\n",
        "  if is_improvement:\n",
        "    best_metric = tuning_metric\n",
        "    n_no_improve = 0\n",
        "\n",
        "  else:\n",
        "    n_no_improve += 1\n",
        "  \n",
        "  save_checkpoint(\n",
        "      {\n",
        "          \"epoch\": epoch + 1,\n",
        "          \"state_dict\": model.state_dict(),\n",
        "          \"optimizer\": optimizer.state_dict(),\n",
        "          \"scheduler\": scheduler.state_dict(),\n",
        "          \"best_metric\": best_metric,\n",
        "      },\n",
        "      is_improvement,\n",
        "      args.savedir,\n",
        "  )\n",
        "\n",
        "  if n_no_improve >= args.patience:\n",
        "    print(\"No improvement. Breaking out of loop.\")\n",
        "    break\n",
        "\n",
        "  print('Train acc: {}'.format(mean_epoch_metric))\n",
        "  print('Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}'\n",
        "  .format(epoch+1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time()-epoch_start_time)))\n",
        "\n",
        "print(\"--- %s seconds ---\" %(time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EXiiqLJr_69",
        "outputId": "83c98782-1e91-47bf-9188-83c834fc1d91"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc: 0.15728345432310775\n",
            "Epoch [1/100], Loss: 6.4091 - Val accuracy: 0.1973 - Epoch time: 69.47\n",
            "Train acc: 0.18265918936851144\n",
            "Epoch [2/100], Loss: 5.5580 - Val accuracy: 0.2137 - Epoch time: 67.30\n",
            "Train acc: 0.18889040668175786\n",
            "Epoch [3/100], Loss: 5.1776 - Val accuracy: 0.2102 - Epoch time: 69.49\n",
            "Train acc: 0.19173504865610283\n",
            "Epoch [4/100], Loss: 4.9315 - Val accuracy: 0.2125 - Epoch time: 67.85\n",
            "Train acc: 0.19671002133583115\n",
            "Epoch [5/100], Loss: 4.7724 - Val accuracy: 0.1960 - Epoch time: 68.66\n",
            "Train acc: 0.20011692425779928\n",
            "Epoch [6/100], Loss: 4.6467 - Val accuracy: 0.2188 - Epoch time: 66.60\n",
            "Train acc: 0.20296359899045094\n",
            "Epoch [7/100], Loss: 4.5422 - Val accuracy: 0.2169 - Epoch time: 69.35\n",
            "Train acc: 0.20515694520334088\n",
            "Epoch [8/100], Loss: 4.4517 - Val accuracy: 0.2194 - Epoch time: 67.22\n",
            "Train acc: 0.20870614120677544\n",
            "Epoch [9/100], Loss: 4.3699 - Val accuracy: 0.2170 - Epoch time: 69.60\n",
            "Train acc: 0.2107787415762496\n",
            "Epoch [10/100], Loss: 4.2956 - Val accuracy: 0.2112 - Epoch time: 67.81\n",
            "Train acc: 0.21317251775817658\n",
            "Epoch [11/100], Loss: 4.2280 - Val accuracy: 0.2121 - Epoch time: 69.10\n",
            "Train acc: 0.21553620911716495\n",
            "Epoch [12/100], Loss: 4.1654 - Val accuracy: 0.2175 - Epoch time: 69.50\n",
            "Train acc: 0.21840117867457653\n",
            "Epoch [13/100], Loss: 4.1061 - Val accuracy: 0.2175 - Epoch time: 68.97\n",
            "Train acc: 0.21966758709962791\n",
            "Epoch [14/100], Loss: 4.0518 - Val accuracy: 0.2225 - Epoch time: 71.13\n",
            "Train acc: 0.22386401334790415\n",
            "Epoch [15/100], Loss: 3.9986 - Val accuracy: 0.2150 - Epoch time: 69.53\n",
            "Epoch 00016: reducing learning rate of group 0 to 5.0000e-03.\n",
            "Train acc: 0.22624274711836184\n",
            "Epoch [16/100], Loss: 3.9477 - Val accuracy: 0.2219 - Epoch time: 70.95\n",
            "Train acc: 0.2360231604350428\n",
            "Epoch [17/100], Loss: 3.8489 - Val accuracy: 0.2257 - Epoch time: 71.30\n",
            "Train acc: 0.23729932609996618\n",
            "Epoch [18/100], Loss: 3.8213 - Val accuracy: 0.2194 - Epoch time: 68.99\n",
            "Train acc: 0.23989068638930086\n",
            "Epoch [19/100], Loss: 3.7982 - Val accuracy: 0.2268 - Epoch time: 70.06\n",
            "Train acc: 0.24222307327036663\n",
            "Epoch [20/100], Loss: 3.7757 - Val accuracy: 0.2197 - Epoch time: 69.11\n",
            "Train acc: 0.2431227720968959\n",
            "Epoch [21/100], Loss: 3.7533 - Val accuracy: 0.2066 - Epoch time: 69.51\n",
            "Train acc: 0.2449351794031171\n",
            "Epoch [22/100], Loss: 3.7316 - Val accuracy: 0.2202 - Epoch time: 68.84\n",
            "Train acc: 0.2459035854604116\n",
            "Epoch [23/100], Loss: 3.7141 - Val accuracy: 0.2167 - Epoch time: 70.66\n",
            "Train acc: 0.24773591379803814\n",
            "Epoch [24/100], Loss: 3.6904 - Val accuracy: 0.2288 - Epoch time: 70.47\n",
            "Train acc: 0.24992072242604013\n",
            "Epoch [25/100], Loss: 3.6715 - Val accuracy: 0.2199 - Epoch time: 69.02\n",
            "Train acc: 0.2508838433117373\n",
            "Epoch [26/100], Loss: 3.6519 - Val accuracy: 0.2215 - Epoch time: 70.24\n",
            "Epoch 00027: reducing learning rate of group 0 to 2.5000e-03.\n",
            "Train acc: 0.25184655764577313\n",
            "Epoch [27/100], Loss: 3.6335 - Val accuracy: 0.2250 - Epoch time: 68.35\n",
            "Train acc: 0.25943362474956416\n",
            "Epoch [28/100], Loss: 3.5859 - Val accuracy: 0.2269 - Epoch time: 69.55\n",
            "Train acc: 0.2594844437072308\n",
            "Epoch [29/100], Loss: 3.5734 - Val accuracy: 0.2250 - Epoch time: 67.99\n",
            "Train acc: 0.2616257513074702\n",
            "Epoch [30/100], Loss: 3.5637 - Val accuracy: 0.2231 - Epoch time: 68.97\n",
            "Train acc: 0.26216280605209064\n",
            "Epoch [31/100], Loss: 3.5558 - Val accuracy: 0.2217 - Epoch time: 69.05\n",
            "Train acc: 0.26282670491504695\n",
            "Epoch [32/100], Loss: 3.5451 - Val accuracy: 0.2236 - Epoch time: 67.61\n",
            "Train acc: 0.26320195209845704\n",
            "Epoch [33/100], Loss: 3.5364 - Val accuracy: 0.2244 - Epoch time: 69.65\n",
            "Train acc: 0.26449803879478573\n",
            "Epoch [34/100], Loss: 3.5279 - Val accuracy: 0.2181 - Epoch time: 67.56\n",
            "Train acc: 0.2661161144068899\n",
            "Epoch [35/100], Loss: 3.5173 - Val accuracy: 0.2257 - Epoch time: 69.96\n",
            "Train acc: 0.2663441898888976\n",
            "Epoch [36/100], Loss: 3.5113 - Val accuracy: 0.2262 - Epoch time: 69.35\n",
            "Train acc: 0.26796307860432445\n",
            "Epoch [37/100], Loss: 3.5002 - Val accuracy: 0.2235 - Epoch time: 70.11\n",
            "Epoch 00038: reducing learning rate of group 0 to 1.2500e-03.\n",
            "Train acc: 0.26832450303124916\n",
            "Epoch [38/100], Loss: 3.4926 - Val accuracy: 0.2223 - Epoch time: 69.77\n",
            "Train acc: 0.271679773892228\n",
            "Epoch [39/100], Loss: 3.4677 - Val accuracy: 0.2251 - Epoch time: 69.41\n",
            "Train acc: 0.2727681126896157\n",
            "Epoch [40/100], Loss: 3.4616 - Val accuracy: 0.2288 - Epoch time: 69.48\n",
            "Train acc: 0.27231765344885905\n",
            "Epoch [41/100], Loss: 3.4598 - Val accuracy: 0.2239 - Epoch time: 69.34\n",
            "Train acc: 0.27345965706554265\n",
            "Epoch [42/100], Loss: 3.4540 - Val accuracy: 0.2241 - Epoch time: 71.00\n",
            "Train acc: 0.2741479490281789\n",
            "Epoch [43/100], Loss: 3.4507 - Val accuracy: 0.2198 - Epoch time: 69.66\n",
            "No improvement. Breaking out of loop.\n",
            "--- 3048.0573139190674 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = NeuralLM(args)\n",
        "best_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/bengio_model/model_best.pt')['state_dict'])\n",
        "best_model.train(False);"
      ],
      "metadata": {
        "id": "bKZimqJQ_4mO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_likelihood(model, text, ngram_model):\n",
        "  X,y = ngram_data.transform([text])\n",
        "  n = X.shape[0]\n",
        "  X,y = X[2:],y[2:]\n",
        "  X = torch.LongTensor(X).unsqueeze(0)\n",
        "  logits = model(X).detach()\n",
        "  probs = F.softmax(logits, dim = 1).numpy()\n",
        "  return n,np.sum([np.log(probs[i][w]) for i,w in enumerate(y)])\n",
        "\n",
        "def Prpx(N,p):\n",
        "  return(np.power(2,-(1/N)*p))"
      ],
      "metadata": {
        "id": "bmcG0qjCAYXe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frases = [\"leo a hegel pero soy materialista\",\"sí estoy de acuerdo compañero \",\n",
        "          \"koolksdjk hb oslskdxxjflskdjf \",\n",
        "          \"chinguen a sus madres cabrones \",\"dios es mi pastor, nada me falta\"]\n",
        "for ph in frases:\n",
        "  print(\"log likelihood de '\"+ph+\"': \", log_likelihood(best_model, ph,ngram_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ4r8vdgAi0I",
        "outputId": "061bf827-e8f2-4f5c-c986-7902b17f0b51"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood de 'leo a hegel pero soy materialista':  (7, -19.083868)\n",
            "log likelihood de 'sí estoy de acuerdo compañero ':  (6, -30.44687)\n",
            "log likelihood de 'koolksdjk hb oslskdxxjflskdjf ':  (4, -3.1049876)\n",
            "log likelihood de 'chinguen a sus madres cabrones ':  (6, -28.797634)\n",
            "log likelihood de 'dios es mi pastor, nada me falta':  (9, -28.738453)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Se carga el mejor modelo y se mide la perplejidad"
      ],
      "metadata": {
        "id": "0e4E1baSkLaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N,P = 0,0\n",
        "for i in X_val:\n",
        "  n,p = log_likelihood(best_model,i,ngram_data)\n",
        "  N += n\n",
        "  P += p\n",
        "Prpx(N,P)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rWchyyfBE2u",
        "outputId": "bda3ea4a-81e4-4506-acb0-be23a873c3a7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17.663239640306976"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### El entrenamiento alcanzó cerca del 0.27 de precisión sobre los datos de entrenamiento y cerca de 0.23 sobre los de validación, resultados mejores que los del modelo hecho en clase, así como la perplejidad (17.66) fue mejor que la del modelo realizado en clase (20.01)."
      ],
      "metadata": {
        "id": "sZ4vekKrkP8m"
      }
    }
  ]
}