{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R583PkvBaeW-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tarea 4: Modelos de lenguaje estadísticos**\n",
        "## Procesamiento de lenguaje natural\n",
        "### Alan García Zermeño\n",
        "10 de Marzo de 2023\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fbhaHT0Jaiv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.- Modelo de lenguaje y evaluación.\n",
        "## 1. Preprocesamiento.\n",
        "### Encontraremos primero las palabras que se repitieron más de 3 veces usando un tokenizador para fijar nuestro vocabulario.\n",
        "### Después usamos la base de datos de tweets de entrenamiento para crear un corpus con palabras que estén en nuestro vocabulario, cambiando todas las que no estén por $\\text{<unk>}$, y los saltos de línea por $\\text{<s>}$ al principio del tweet y $\\text{</s>}$ al final.\n"
      ],
      "metadata": {
        "id": "2zXFck1u-R-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_from_file(path_corpus, path_truth):\n",
        "  tr_txt = []\n",
        "  tr_y = []\n",
        "\n",
        "  with open(path_corpus,\"r\") as f_corpus, open(path_truth, \"r\") as f_truth:\n",
        "    for twitt in f_corpus:\n",
        "      tr_txt += [twitt]\n",
        "    for label in f_truth:\n",
        "      tr_y += [label]\n",
        "  \n",
        "  return tr_txt,tr_y"
      ],
      "metadata": {
        "id": "QJGmp4AhaiOS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_txt, tr_y = get_text_from_file(\"/content/drive/MyDrive/Colab Notebooks/mex_data/mex_train.txt\",\"/content/drive/MyDrive/Colab Notebooks/mex_data/mex_train_labels.txt\")\n",
        "test_txt, test_y = get_text_from_file(\"/content/drive/MyDrive/Colab Notebooks/mex_data/mex_val.txt\",\"/content/drive/MyDrive/Colab Notebooks/mex_data/mex_val_labels.txt\")"
      ],
      "metadata": {
        "id": "SQdEM_bna2Zo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tokenizer = TweetTokenizer()"
      ],
      "metadata": {
        "id": "DNGj68LCa447"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_palabras = []\n",
        "for doc in tr_txt:\n",
        "  corpus_palabras += tokenizer.tokenize(doc)\n",
        "\n",
        "#corpus_palabras = [t for t in corpus_palabras if t not in stopwords]\n",
        "fdist = nltk.FreqDist(corpus_palabras)\n",
        "len(fdist)"
      ],
      "metadata": {
        "id": "1F4BJyJh7Lsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc9318d-42e4-41e8-b944-62e01eb5d4f7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13581"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defino mi vocabulario\n",
        "V = [key for key in fdist if key[0] not in [\"#\",\"/\"] and fdist[key] > 2]\n",
        "V.append('</s>')\n",
        "V.append('<s>')\n",
        "V.append('<unk>')\n",
        "len(V)"
      ],
      "metadata": {
        "id": "tW_rFnuE7nkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e9b94c-57e0-492e-a19b-a69caa660cc7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3102"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preproces(txt):\n",
        "  corpus = []\n",
        "  for doc in txt:\n",
        "    corpus.append(\"<s>\")\n",
        "    for word in tokenizer.tokenize(doc):\n",
        "      if word not in V:\n",
        "        corpus.append(\"<unk>\")\n",
        "      else:\n",
        "        corpus.append(word)\n",
        "    corpus.append(\"</s>\")\n",
        "  return(corpus)"
      ],
      "metadata": {
        "id": "Qx46cRCZEYKR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_txt_val = tr_txt[len(tr_txt)-len(test_txt):] #Validation data\n",
        "tr_txt = tr_txt[:len(tr_txt)-len(test_txt)] #Training data\n",
        "\n",
        "corp_prepro = preproces(tr_txt)\n",
        "prepro_val = preproces(tr_txt_val)\n",
        "prepro_test = preproces(test_txt)\n",
        "#txt_corp = nltk.text.Text(corp_prepro)\n",
        "print(corp_prepro[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU5gkeOwAGvd",
        "outputId": "cf4756a3-9199-42f0-bb9e-96684e13d064"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'lo', 'peor', 'de', 'todo', 'es', 'que', 'no', 'me', 'dan', 'por', 'un', 'tiempo', 'y', 'luego', 'vuelven', 'estoy', 'hasta', 'la', 'verga', 'de', '<unk>', '</s>', '<s>', 'a', 'la', 'vga', 'no', 'seas', 'mamón']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Modelo de N-gramas"
      ],
      "metadata": {
        "id": "m5tvXfzdWE_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def c_base(corp,words):\n",
        "  return(0)"
      ],
      "metadata": {
        "id": "hczYXoTCEQrS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_cor = nltk.FreqDist(corp_prepro)   #Frecuencia de unigramas\n",
        "\n",
        "def c_uni(corp,word):\n",
        "  return(fdist_cor[word[0]])"
      ],
      "metadata": {
        "id": "vyTTOBNkXo67"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tt = nltk.bigrams(corp_prepro)\n",
        "fdist_cor_bi = nltk.FreqDist(tt)  #frecuencia de bigramas\n",
        "\n",
        "def c_big(corp,words):\n",
        "  return(fdist_cor_bi[(words[0],words[1])])"
      ],
      "metadata": {
        "id": "fHiKhiBbiXzo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find = nltk.collocations.TrigramCollocationFinder.from_words(corp_prepro)\n",
        "trigrams = find.ngram_fd  #Frecuencia de trigramas"
      ],
      "metadata": {
        "id": "NgG1wQBIR4pB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def c_tri(corp,words):\n",
        "  coun = trigrams[(words[0],words[1],words[2])]\n",
        "  return(coun)"
      ],
      "metadata": {
        "id": "m41fMgHsR7se"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def P_markov(corp,phrase,ngram,n):\n",
        "  g = [c_base,c_uni,c_big,c_tri]\n",
        "  ph = phrase#.split()\n",
        "  p = 0\n",
        "  kk = 0.01\n",
        "  m = n*kk\n",
        "\n",
        "  if len(ph) < ngram:\n",
        "    print(\"Error\")\n",
        "    return 0\n",
        "\n",
        "  #Check for missing words in vocabulary and replace\n",
        "  for i in range(len(ph)):\n",
        "    if ph[i] not in V:\n",
        "      ph[i] = \"<unk>\"\n",
        "  \n",
        "  #Compute ngram markov chain probability\n",
        "  for i in range(len(ph)-ngram+1):\n",
        "      #Estas condiciones son para casos que no vale la pena contar\n",
        "      if ngram > 1 and ph[i:i+ngram][0] == \"</s>\":\n",
        "        pass\n",
        "      if ngram == 3 and ph[i:i+ngram][1] in [\"<s>\",\"</s>\"]:\n",
        "        pass\n",
        "      else:    \n",
        "        if ngram > 1:\n",
        "          pn = g[ngram](corp,ph[i:i+ngram])\n",
        "          p += np.log2((pn+kk)/(g[ngram-1](corp,ph[i:i+ngram-1]) + m))\n",
        "        else:\n",
        "          p += np.log2((c_uni(corp,[ph[i]])+kk)/(n+m))\n",
        "          #print(np.power(2,np.log2((c_uni(corp,[ph[i]])+kk)/m)),m,n)\n",
        "    #print(\"c(\",ph[i:i+ngram],\") =\",pn)\n",
        "\n",
        "  return(p) "
      ],
      "metadata": {
        "id": "Rvbw2hpU-d5e"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se optó por un suavizado de Laplace para lidiar con los conteos nulos y para el uso de la función P_markov enviamos en forma de arreglo la cadena a la cual queremos calcular su probabilidad, y con el último argumento definimos si queremos usar unigramas (1), bigramas(2) o trigramas (3). La función devuelve el logaritmo base 2 de la probabilidad calculada, por lo que calculamos $2^{\\log_2 p}$, para encontrar la probabilidad de 0 a 1."
      ],
      "metadata": {
        "id": "YHbK6jgbq-Gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_uni(prepro_val, [\"psicólogo\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH3gfHDIgkXa",
        "outputId": "a9d2f453-0bb4-461c-a077-1e2352829ac3"
      },
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 352
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c_big(prepro_val,[\"viva\",\"méxico\"])"
      ],
      "metadata": {
        "id": "KjLwos_tiNkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e51567-b3cf-48cc-d6fa-e837c47a2db7"
      },
      "execution_count": 353,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 353
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c_tri(prepro_val,[\"viva\",\"méxico\",\"</s>\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXaIdjXtSrcH",
        "outputId": "7765ba46-866a-4fd3-eebf-452e391e2fec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex1 = P_markov(corp_prepro, [\"viva\" ,\"méxico\" ,\"</s>\"],1,len(corp_prepro))\n",
        "ex2 = P_markov(corp_prepro, [\"viva\" ,\"méxico\" ,\"</s>\"],2,len(corp_prepro))\n",
        "ex3 = P_markov(corp_prepro, [\"viva\" ,\"méxico\" ,\"</s>\"],3,len(corp_prepro))\n",
        "print(\"P_uni = \",np.power(2,ex1),\"\\nP_big = \",np.power(2,ex2) ,\"\\nP_trig = \",np.power(2,ex3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20-KV7kgFY7l",
        "outputId": "00b6e279-bdf1-45fb-bd11-8ca5f864d805"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P_uni =  4.3015661126774835e-09 \n",
            "P_big =  2.59862107786755e-05 \n",
            "P_trig =  0.0010035372203012593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex1 = P_markov(corp_prepro, [\"mi\" ,\"psicólogo\",\"dice\"],1,len(corp_prepro))\n",
        "ex2 = P_markov(corp_prepro, [\"mi\" ,\"psicólogo\",\"dice\"],2,len(corp_prepro))\n",
        "ex3 = P_markov(corp_prepro, [\"mi\" ,\"psicólogo\",\"dice\"],3,len(corp_prepro))\n",
        "print(\"P_uni = \",np.power(2,ex1),\"\\nP_big = \",np.power(2,ex2) ,\"\\nP_trig = \",np.power(2,ex3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-9CNwA4sbcm",
        "outputId": "b8e5d7cc-3ad1-4d07-c545-34acf9f2b669"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P_uni =  4.6245754568951226e-07 \n",
            "P_big =  5.277325231034305e-05 \n",
            "P_trig =  0.0009144906015718378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Análisis de Perplejidad con un modelo $\\hat{P}$ interpolado de la forma:\n",
        "## $\\hat{P}(w_n|w_{n-2}w_{n-1}) = \\lambda_1 P(w_n|w_{n-2}w_{n-1}) + \\lambda_2 P(w_n|w_{n-1}) + \\lambda_3 P(w_n)$\n",
        "## $Prp = 2^{-\\frac{1}{N}∑_i^n \\log_2 ~p_i}$"
      ],
      "metadata": {
        "id": "zq-m_e6ysw7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Prpx(N,p):\n",
        "  return(np.power(2,-(1/N)*p))"
      ],
      "metadata": {
        "id": "ABrmzD7FZ7v1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sobre datos de validación\n",
        "N = len(prepro_val)\n",
        "ei = np.array([P_markov(corp_prepro, prepro_val,1,len(V)),\n",
        "               P_markov(corp_prepro, prepro_val,2,len(V)),\n",
        "               P_markov(corp_prepro, prepro_val,3,len(V))])\n",
        "\n",
        "#Pi = np.power(2,ei[0])\n",
        "lamb = np.array([[1/3,1/3,1/3],[0.4,0.4,0.2],[0.2,0.4,0.4],\n",
        "                 [0.5,0.4,0.1],[0.1,0.4,0.5]])\n",
        "\n",
        "P_interp = [np.sum(lam*ei) for lam in lamb]\n",
        "print(\"Perplejidad de interpolación con distintos valores de lambda:\\n\")\n",
        "print(Prpx(N,ei))\n",
        "for i,p in enumerate(P_interp):\n",
        "  print(\"Prpx(lam = \",lamb[i],\") = \", Prpx(N,p))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q6d2i01-33R",
        "outputId": "1b0e86f4-76f8-4234-8b63-a461314319ca"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplejidad de interpolación con distintos valores de lambda:\n",
            "\n",
            "[  7.33534972 142.14086186 341.86752198]\n",
            "Prpx(lam =  [0.33333333 0.33333333 0.33333333] ) =  70.90320565347504\n",
            "Prpx(lam =  [0.4 0.4 0.2] ) =  51.7639580016772\n",
            "Prpx(lam =  [0.2 0.4 0.4] ) =  111.61302535542941\n",
            "Prpx(lam =  [0.5 0.4 0.1] ) =  35.25200103261191\n",
            "Prpx(lam =  [0.1 0.4 0.5] ) =  163.89231214403225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sobre datos de prueba\n",
        "N = len(prepro_test)\n",
        "ei = np.array([P_markov(corp_prepro, prepro_test,1,len(V)),\n",
        "               P_markov(corp_prepro, prepro_test,2,len(V)),\n",
        "               P_markov(corp_prepro, prepro_test,3,len(V))])\n",
        "\n",
        "#Pi = np.power(2,ei[0])\n",
        "lamb = np.array([[1/3,1/3,1/3],[0.4,0.4,0.2],[0.2,0.4,0.4],\n",
        "                 [0.5,0.4,0.1],[0.1,0.4,0.5]])\n",
        "\n",
        "P_interp = [np.sum(lam*ei) for lam in lamb]\n",
        "print(Prpx(N,ei))\n",
        "print(\"Perplejidad de interpolación con distintos valores de lambda:\\n\")\n",
        "for i,p in enumerate(P_interp):\n",
        "  print(\"Prpx(lam = \",lamb[i],\") = \", Prpx(N,p))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bVzlbtVxfHa",
        "outputId": "51239091-7753-4bc2-93d0-b668e99e0a2b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  6.09775253 120.68490018 293.85119705]\n",
            "Perplejidad de interpolación con distintos valores de lambda:\n",
            "\n",
            "Prpx(lam =  [0.33333333 0.33333333 0.33333333] ) =  60.02286642828897\n",
            "Prpx(lam =  [0.4 0.4 0.2] ) =  43.687237647332196\n",
            "Prpx(lam =  [0.2 0.4 0.4] ) =  94.83008043029005\n",
            "Prpx(lam =  [0.5 0.4 0.1] ) =  29.65232954908213\n",
            "Prpx(lam =  [0.1 0.4 0.5] ) =  139.71463027942588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sobre los datos de validación y sobre los de test el modelo interpolado de $\\lambda = [0.5,0.4,0.1]$ funcionó mejor, así como en los datos de prueba. En ambos casos vemos que el modelo de unigramas fue el que más probabilidad calculó.\n",
        "### Se esperaba que la perplejidad de los trigramas fuera menor, sin embargo pudo haber afectado el uso del suavizado de Laplace. Previamente se hicieron pruebas contando combinaciones de tokens como c($</s>$,palabra), pero la perplejidad de los trigramas y bigramas bajó bastante cuando se dejaron de contar."
      ],
      "metadata": {
        "id": "kXm4PWDCuK7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Generación de texto"
      ],
      "metadata": {
        "id": "Ne9L7zD5vC9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Usando valores fijos de $\\lambda$, generamos tweets de máximo 50 palabras, sampleando sobre un arreglo de las posibles palabras a generar según los conteos de bigramas y seleccionando la siguiente palabra con base en las probabilidades interpoladas de cada una de ellas.\n",
        "## Para ello, usamos valores de $\\lambda$ que le quiten peso a los unigramas. Por ello (y por falta de tiempo), no se programó la función de EM, ya que, por las perplejidades calculadas previamente, éste algoritmo iba a darle todo el peso a los unigramas.\n",
        "## Imprimimos 5 tweets generados con 2 palabras de interés cada uno para empezar a generar el tweet."
      ],
      "metadata": {
        "id": "-xaaLsv-Rcqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lamb1 = 0.1\n",
        "lamb2 = 0.4\n",
        "lamb3 = 0.5\n",
        "\n",
        "def tuitear(corp,iter,tw):\n",
        "  no = [\"</s>\",\"<s>\",\"<unk>\",\"|\",\"'\",'\"',\"¡\",\"¿\",\".\"]\n",
        "  p = {}\n",
        "\n",
        "  for jj in range(iter):\n",
        "    p.clear()\n",
        "    if jj == 30:\n",
        "      no.pop(0)\n",
        "\n",
        "    #Creamos una lista de palabras candidatas\n",
        "    for word in V:\n",
        "      if word not in no:      \n",
        "        big = c_big(corp,[tw[-1],word])\n",
        "        if big > 1:\n",
        "          p[word] = big\n",
        "    #print(len(p),tw[-1])\n",
        "\n",
        "    for word in p:\n",
        "      uni = P_markov(corp,[word],1,len(corp))\n",
        "      big = P_markov(corp,[tw[-1],word],2,len(corp))\n",
        "      tri = P_markov(corp,tw[-2:]+[word],3,len(corp))\n",
        "      pg = lamb1*np.power(2,uni) + lamb2*np.power(2,big) + lamb3*np.power(2,tri)\n",
        "      p[word] = pg\n",
        "\n",
        "    sum = np.sum(list(p.values()))\n",
        "    t = 0\n",
        "    r = np.random.uniform(0,1)\n",
        "    #print(len(p),tw[-1])\n",
        "\n",
        "    for w in p:\n",
        "      p[w] = p[w]/sum + t\n",
        "      if r < p[w] and r > t:\n",
        "        if w == \"</s>\":\n",
        "          return \" \".join(tw)\n",
        "        tw.append(w)\n",
        "        break\n",
        "      t = p[w]\n",
        "  return(\" \".join(tw))"
      ],
      "metadata": {
        "id": "whFx-PmvmbY1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1.- \",tuitear(corp_prepro,50,[\"no\",\"creo\"]))\n",
        "print(\"2.- \",tuitear(corp_prepro,50,[\"yo\",\"soy\"]))\n",
        "print(\"3.- \",tuitear(corp_prepro,50,[\"estoy\",\"bien\"]))\n",
        "print(\"4.- \",tuitear(corp_prepro,50,[\"te\",\"voy\"]))\n",
        "print(\"5.- \",tuitear(corp_prepro,50,[\"me\",\"gusta\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noEtHQzBGSz5",
        "outputId": "87acb062-2f97-42df-bae8-24800751a657"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.-  no creo que se nos va a putas rotaciones\n",
            "2.-  yo soy el día para qué putas ratas que me digan esos putos corruptos de la madre porque putas ! el no mamen\n",
            "3.-  estoy bien la lengua en 8 putos narizones de esa madre yo siempre está su puta madre el hijo de puta madre ... ni a su novio el mundial a la vida y todos los de “ es que tengo en la madre ! ! por qué putas van a nadie más\n",
            "4.-  te voy a la escuela de un hombre se quejan\n",
            "5.-  me gusta de honduras y que me cagan las mujeres : v ) ... para mandarte a los putos argentinos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Datos de la mañanera de AMLO\n",
        "### Se tomaron ahora los datos de la mañanera de AMLO, debido al tamaño del corpus, todo el preprocesamiento resultó muy tardado. Pero se usó el mismo método para definir el vocabulario que en el corpus de los tweets (Palabras repetidas más de n ocasiones).\n"
      ],
      "metadata": {
        "id": "I9yRfSUb8c9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "amloConf = \"\"\n",
        "for f_pagina in glob.glob(\"/content/drive/MyDrive/Colab Notebooks/estenograficas_limpias_por_fecha/*\"):\n",
        "    amloConf += (open(f_pagina,\"r\",encoding = \"utf-8\").read())[800:]\n",
        "\n",
        "amloConfTok = amloConf.split()\n",
        "len(amloConfTok)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWYhCVaZn1_X",
        "outputId": "4ea9d172-7513-4fe2-c903-bf16ce9877a8"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11001741"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "amloConfTok = tokenizer.tokenize(amloConf)\n",
        "fdist_AMLO = nltk.FreqDist(amloConfTok)\n",
        "len(fdist_AMLO)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZDmEzWEAaK3",
        "outputId": "01f13e2e-76f6-4023-957e-edfcf54949a4"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94324"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V_AMLO = [key for key in fdist_AMLO if key[0] not in [\"#\",\"/\"] and fdist_AMLO[key] > 4]\n",
        "#V_AMLO.append('</s>')\n",
        "#V_AMLO.append('<s>')\n",
        "V_AMLO.append('<unk>')\n",
        "len(V_AMLO)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzQ63hM3BDTB",
        "outputId": "00b47f16-8370-498b-c718-317e42a5dd2d"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37737"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preproces_txt(txt):\n",
        "  corpus = []\n",
        "    #corpus.append(\"<s>\")\n",
        "  for word in txt:\n",
        "    if word not in V_AMLO:\n",
        "      corpus.append(\"<unk>\")\n",
        "    else:\n",
        "      corpus.append(word)\n",
        "    #corpus.append(\"</s>\")\n",
        "  return(corpus)"
      ],
      "metadata": {
        "id": "MNs8qFmVDpN6"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AMLO_prepro = preproces_txt(amloConfTok)\n",
        "AMLO_prepro[:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ-NnlvEFT52",
        "outputId": "c90143da-9411-428d-9723-38685fe0adc5"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>',\n",
              " '.',\n",
              " '18',\n",
              " 'Versión',\n",
              " 'estenográfica',\n",
              " 'de',\n",
              " 'la',\n",
              " 'firma',\n",
              " 'del',\n",
              " 'Decreto',\n",
              " 'de',\n",
              " 'Estímulos',\n",
              " '<unk>',\n",
              " 'de',\n",
              " 'la',\n",
              " 'Región',\n",
              " '<unk>',\n",
              " 'Norte',\n",
              " 'VE',\n",
              " '<unk>',\n",
              " 'Versión',\n",
              " 'estenográfica',\n",
              " 'San',\n",
              " 'Pedro',\n",
              " 'Garza',\n",
              " 'García',\n",
              " ',',\n",
              " 'Nuevo',\n",
              " 'León',\n",
              " ',']"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_cor = nltk.FreqDist(AMLO_prepro)   #Frecuencia de unigramas\n",
        "tt = nltk.bigrams(AMLO_prepro)\n",
        "fdist_cor_bi_AM = nltk.FreqDist(tt)  #frecuencia de bigramas\n",
        "\n",
        "find = nltk.collocations.TrigramCollocationFinder.from_words(AMLO_prepro)\n",
        "trigrams_AM = find.ngram_fd  #Frecuencia de trigramas"
      ],
      "metadata": {
        "id": "KfnsRbyjGsFz"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def c_uni(corp,word):\n",
        "  return(fdist_cor[word[0]])\n",
        "\n",
        "def c_big(corp,words):\n",
        "  return(fdist_cor_bi_AM[(words[0],words[1])])\n",
        "\n",
        "def c_tri(corp,words):\n",
        "  coun = trigrams_AM[(words[0],words[1],words[2])]\n",
        "  return(coun)"
      ],
      "metadata": {
        "id": "iwEXjifdDbMk"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Primero usamos la misma estrategia de generación de tweets para generar una conferencia de 300 palabras iniciada por \"Buenos días\". El resultado no es suficientemente satisfactorio, podemos encontrar pequeñas frases conexas y con sentido pero en general no podemos decir que es un buen modelo."
      ],
      "metadata": {
        "id": "zBN05HajUcon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lamb1 = 0.1\n",
        "lamb2 = 0.4\n",
        "lamb3 = 0.5\n",
        "\n",
        "def conference(corp,iter):\n",
        "  tw = [\"buenos\",\"días\"]\n",
        "  no = [\"</s>\",\"<s>\",\"<unk>\",\"|\",\"'\",'\"',\"¡\",\"¿\"]\n",
        "  p = {}\n",
        "\n",
        "  for jj in range(iter):\n",
        "    p.clear()\n",
        "\n",
        "    #Creamos una lista de palabras candidatas\n",
        "    for word in V_AMLO:\n",
        "      if word not in no:      \n",
        "        big = c_big(AMLO_prepro,[tw[-1],word])\n",
        "        pg = np.power(2,big)\n",
        "        if pg > 3:\n",
        "          p[word] = pg\n",
        "    \n",
        "    #Le interpolamos una probabilidad a cada una\n",
        "    for word in p:\n",
        "      uni = P_markov(corp,[word],1,len(AMLO_prepro))\n",
        "      big = P_markov(corp,[tw[-1],word],2,len(AMLO_prepro))\n",
        "      tri = P_markov(corp,tw[-2:]+[word],3,len(AMLO_prepro))\n",
        "      p[word] = lamb1*np.power(2,uni) + lamb2*np.power(2,big) + lamb3*np.power(2,tri)\n",
        "\n",
        "    #Normalizamos y sampleamos una de las palabras\n",
        "    sum = np.sum(list(p.values()))\n",
        "    t = 0\n",
        "    r = np.random.uniform(0,1)\n",
        "    for w in p:\n",
        "      p[w] = p[w]/sum + t\n",
        "      if r < p[w] and r > t:\n",
        "        tw.append(w)\n",
        "        break\n",
        "      t = p[w]\n",
        "\n",
        "  return(\" \".join(tw))"
      ],
      "metadata": {
        "id": "tVBuPAzEJEzS"
      },
      "execution_count": 344,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conference(AMLO_prepro,300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "f99Y1JUwJp7s",
        "outputId": "8b192c4a-e828-4cc4-feb5-ecd2f879c34f"
      },
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'buenos días vimos las regalías , desató la conquista a Múzquiz . Empresas que subirle , colosal , valía el carbón de Evo a especializarse en eficiencia en grado . BLAS : Sé de vanidad , igualdad … VOZ : Cassez y clasismo . Tomen nota Jesús si regresaran a detallar . INTELOCUTOR : liberales en francos , predominó por influencias de Cristo , protestaron , hidráulica y cometen actos heroicos de previsiones de cierres y dinamizar la Panamericana y CorpoGas , válido y casinos . Hago notar la Farmacia Benavides y cimentación de instalarse . Chichén Itzá , facilidades , empiecen , conseguir alimentos es Elektra y numerario , protegido a Conapesca para secretario se polarizó , aduanas no hará la deducción de directora . Pudiera pensarse que asistir en tratos y levantaban la eliminación , asesinatos en Relaciones que definirnos , 508 , Maribel Villegas y distribuyendo . EDGAR JESÚS CRUZ ESCANDÓN CADENAS , poniéndose de 114 en recorrido de excavaciones . EDUARDO REDONDO ARÁMBURO , procuremos que acostumbrarnos a Sodi , Donald'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 345
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Calculamos ahora la probabilidad para nuestros dos modelos, 2 frases distintas: \n",
        "## *sino gano me voy a la chingada*"
      ],
      "metadata": {
        "id": "H7Je_FlYUnDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de AMLO"
      ],
      "metadata": {
        "id": "Gm1ql9vOVT0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ex1 = P_markov(AMLO_prepro, [\"sino\" ,\"gano\" ,\"me\",\"voy\",\"a\",\"la\",\"chingada\"],1,len(AMLO_prepro))\n",
        "ex2 = P_markov(AMLO_prepro, [\"sino\" ,\"gano\" ,\"me\",\"voy\",\"a\",\"la\",\"chingada\"],2,len(AMLO_prepro))\n",
        "ex3 = P_markov(AMLO_prepro, [\"sino\" ,\"gano\" ,\"me\",\"voy\",\"a\",\"la\",\"chingada\"],3,len(AMLO_prepro))\n",
        "print(\"P_uni = \",np.power(2,ex1),\"\\nP_big = \",np.power(2,ex2) ,\"\\nP_trig = \",np.power(2,ex3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on9-lkIlHIGV",
        "outputId": "6347e8cd-c786-419d-d22c-d3e78b931d53"
      },
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P_uni =  1.1854752675745273e-24 \n",
            "P_big =  1.1834976981245866e-20 \n",
            "P_trig =  6.761651376528221e-28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de tweets"
      ],
      "metadata": {
        "id": "HgqaWSudVWuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ex1 = P_markov(corp_prepro, [\"sino\" ,\"gano\" ,\"me\",\"voy\",\"a\",\"la\",\"chingada\"],1,len(corp_prepro))\n",
        "ex2 = P_markov(corp_prepro, [\"sino\" ,\"gano\" ,\"me\",\"voy\",\"a\",\"la\",\"chingada\"],2,len(corp_prepro))\n",
        "ex3 = P_markov(corp_prepro, [\"sino\" ,\"gano\" ,\"me\",\"voy\",\"a\",\"la\",\"chingada\"],3,len(corp_prepro))\n",
        "print(\"P_uni = \",np.power(2,ex1),\"\\nP_big = \",np.power(2,ex2) ,\"\\nP_trig = \",np.power(2,ex3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhEpu01jGM5S",
        "outputId": "adf9bde2-50b0-4580-e814-894a0b27452a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P_uni =  4.764022901954099e-17 \n",
            "P_big =  1.3218064657063327e-13 \n",
            "P_trig =  9.228839269547681e-17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *ya se va a acabar la corrupción*"
      ],
      "metadata": {
        "id": "5QWBc6wdVdse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de AMLO"
      ],
      "metadata": {
        "id": "E6YgrPMzVjqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ex1 = P_markov(AMLO_prepro, [\"ya\" ,\"se\" ,\"va\",\"a\",\"acabar\",\"la\",\"corrupción\"],1,len(AMLO_prepro))\n",
        "ex2 = P_markov(AMLO_prepro, [\"ya\" ,\"se\" ,\"va\",\"a\",\"acabar\",\"la\",\"corrupción\"],2,len(AMLO_prepro))\n",
        "ex3 = P_markov(AMLO_prepro, [\"ya\" ,\"se\" ,\"va\",\"a\",\"acabar\",\"la\",\"corrupción\"],3,len(AMLO_prepro))\n",
        "print(\"P_uni = \",np.power(2,ex1),\"\\nP_big = \",np.power(2,ex2) ,\"\\nP_trig = \",np.power(2,ex3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHnmPagFODeN",
        "outputId": "54760087-877b-4d30-cf8f-e0f8faed57ed"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P_uni =  2.9866736314444386e-18 \n",
            "P_big =  9.125692563030109e-14 \n",
            "P_trig =  5.068400071230098e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de tweets"
      ],
      "metadata": {
        "id": "_8LauzCaVmZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ex1 = P_markov(corp_prepro, [\"ya\" ,\"se\" ,\"va\",\"a\",\"acabar\",\"la\",\"corrupción\"],1,len(corp_prepro))\n",
        "ex2 = P_markov(corp_prepro, [\"ya\" ,\"se\" ,\"va\",\"a\",\"acabar\",\"la\",\"corrupción\"],2,len(corp_prepro))\n",
        "ex3 = P_markov(corp_prepro, [\"ya\" ,\"se\" ,\"va\",\"a\",\"acabar\",\"la\",\"corrupción\"],3,len(corp_prepro))\n",
        "print(\"P_uni = \",np.power(2,ex1),\"\\nP_big = \",np.power(2,ex2) ,\"\\nP_trig = \",np.power(2,ex3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZUX_W08GWNq",
        "outputId": "72cc06bc-d505-4705-d376-8407973980cf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P_uni =  1.171122011735223e-19 \n",
            "P_big =  3.055792160215833e-17 \n",
            "P_trig =  2.6902840572272688e-20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Por último, calculamos la probabilidad de todas las permutaciones posibles de ambas frases y mostramos el top 5 de permutaciones más probables.\n",
        "## *ya se va a acabar la corrupción*"
      ],
      "metadata": {
        "id": "BlU9y5kaVuIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "def permutations(corp,chain,n,k):\n",
        "  d = {}\n",
        "  perm = list(itertools.permutations(chain))\n",
        "  for prm in perm:\n",
        "    ex1 = np.power(2,P_markov(corp,list(prm),1,n))\n",
        "    ex2 = np.power(2,P_markov(corp,list(prm),2,n))\n",
        "    ex3 = np.power(2,P_markov(corp,list(prm),3,n))\n",
        "    d[prm] = lamb1*ex1 + lamb2*ex2 + lamb3*ex3\n",
        "  d = dict(sorted(d.items(), key=lambda item: item[1],reverse=True))\n",
        "  return([(list(d)[i],list(d.values())[i]) for i in range(k)])"
      ],
      "metadata": {
        "id": "2461sIGGR28L"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de AMLO"
      ],
      "metadata": {
        "id": "-7HERSp3WFSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = permutations(AMLO_prepro,[\"ya\" ,\"se\" ,\"va\",\"a\",\"acabar\",\"la\",\"corrupción\"],len(AMLO_prepro),5)\n",
        "for i in g: print(i[0],i[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yjc-nj-PTD0j",
        "outputId": "10d4ebe6-c83c-493a-f56b-aa22bb41150e"
      },
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ya', 'se', 'va', 'a', 'acabar', 'la', 'corrupción') 2.7631095027378115e-14\n",
            "('ya', 'la', 'corrupción', 'se', 'va', 'a', 'acabar') 1.773150099266756e-14\n",
            "('la', 'corrupción', 'ya', 'se', 'va', 'a', 'acabar') 6.909040387394665e-15\n",
            "('acabar', 'la', 'corrupción', 'ya', 'se', 'va', 'a') 5.877799493567909e-15\n",
            "('acabar', 'ya', 'se', 'va', 'a', 'la', 'corrupción') 2.4885907121919703e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de Tweets"
      ],
      "metadata": {
        "id": "dJgbOztzWHEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = permutations(corp_prepro,[\"ya\" ,\"se\" ,\"va\",\"a\",\"acabar\",\"la\",\"corrupción\"],len(corp_prepro),5)\n",
        "for i in g: print(i[0],i[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZigYvGT8WIz3",
        "outputId": "994eb1ec-1da2-4eb8-f7d6-bc04bc7ae127"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ya', 'se', 'va', 'a', 'la', 'corrupción', 'acabar') 4.200989711920996e-15\n",
            "('acabar', 'ya', 'se', 'va', 'a', 'la', 'corrupción') 4.1884858738444555e-15\n",
            "('ya', 'a', 'la', 'corrupción', 'se', 'va', 'acabar') 2.8547078683751045e-15\n",
            "('a', 'la', 'corrupción', 'ya', 'se', 'va', 'acabar') 1.8058786830128012e-16\n",
            "('ya', 'se', 'va', 'acabar', 'a', 'la', 'corrupción') 1.8005015007962087e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *sino gano me voy a la chingada*"
      ],
      "metadata": {
        "id": "8cA6k_IwX5wy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de AMLO"
      ],
      "metadata": {
        "id": "AMbanxPyX-v_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = permutations(AMLO_prepro,[\"sino\" ,\"gano\" ,\"me\",\"voy\",\"a\",\"la\",\"chingada\"],len(AMLO_prepro),5)\n",
        "for i in g: print(i[0],i[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhVAZG7BYW67",
        "outputId": "05bfd1b9-098f-40ae-e9fe-1582875787e1"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('chingada', 'sino', 'me', 'voy', 'a', 'la', 'gano') 1.4028522006077076e-19\n",
            "('sino', 'me', 'voy', 'a', 'la', 'gano', 'chingada') 7.955677340304373e-20\n",
            "('chingada', 'me', 'voy', 'a', 'la', 'gano', 'sino') 1.8496458922101176e-20\n",
            "('chingada', 'sino', 'me', 'voy', 'a', 'gano', 'la') 1.825893067711217e-20\n",
            "('me', 'voy', 'a', 'la', 'gano', 'sino', 'chingada') 1.775165275272428e-20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de Tweets"
      ],
      "metadata": {
        "id": "QTHXgFtEYAbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = permutations(corp_prepro,[\"sino\" ,\"gano\" ,\"me\",\"voy\",\"a\",\"la\",\"chingada\"],len(corp_prepro),5)\n",
        "for i in g: print(i[0],i[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7YZshVJUY9_",
        "outputId": "2cda0248-971d-477a-d5d3-d4c6dc85c12b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('me', 'voy', 'a', 'gano', 'sino', 'la', 'chingada') 2.954160836354483e-13\n",
            "('me', 'voy', 'a', 'la', 'chingada', 'gano', 'sino') 1.7468882169312703e-13\n",
            "('sino', 'la', 'chingada', 'a', 'gano', 'me', 'voy') 1.6200583269956476e-13\n",
            "('sino', 'la', 'chingada', 'gano', 'me', 'voy', 'a') 9.732983789052444e-14\n",
            "('sino', 'me', 'voy', 'a', 'gano', 'la', 'chingada') 9.300765819761783e-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Al observar las probabilidades de las permutaciones, comprobamos que los modelos no son suficientemente buenos para contemplar frases largas, las probabilidades de los trigramas y bigramas más comunes hacen que las permutaciones menos lógicas tengan mayor probabilidad."
      ],
      "metadata": {
        "id": "mcLCf1mPZT4j"
      }
    }
  ]
}
